{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets peft wandb scikit-learn scikit-multilearn\n!pip install -i https://pypi.org/simple/ bitsandbytes\n!pip install accelerate","metadata":{"execution":{"iopub.status.busy":"2024-05-19T21:43:29.607226Z","iopub.execute_input":"2024-05-19T21:43:29.608212Z","iopub.status.idle":"2024-05-19T21:44:14.791878Z","shell.execute_reply.started":"2024-05-19T21:43:29.608163Z","shell.execute_reply":"2024-05-19T21:44:14.790834Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\nCollecting peft\n  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.6)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: scikit-multilearn in /opt/conda/lib/python3.10/site-packages (0.2.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.29.3)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.45.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.11.1\nLooking in indexes: https://pypi.org/simple/\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.1\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.29.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.22.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport random\nimport functools\nimport csv\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import f1_score\nfrom skmultilearn.model_selection import iterative_train_test_split\nfrom transformers import AutoTokenizer, AutoModel\nfrom datasets import DatasetDict, Dataset\nfrom datasets import Dataset, DatasetDict\nfrom peft import PeftModel, PeftConfig\nfrom peft import (\n    LoraConfig,\n    prepare_model_for_kbit_training,\n    get_peft_model\n)\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    Trainer\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T21:48:00.699007Z","iopub.execute_input":"2024-05-19T21:48:00.699395Z","iopub.status.idle":"2024-05-19T21:48:17.804020Z","shell.execute_reply.started":"2024-05-19T21:48:00.699365Z","shell.execute_reply":"2024-05-19T21:48:17.803010Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-05-19 21:48:10.097462: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-19 21:48:10.097606: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-19 21:48:10.203190: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-19T21:51:54.210422Z","iopub.execute_input":"2024-05-19T21:51:54.211421Z","iopub.status.idle":"2024-05-19T21:51:54.216628Z","shell.execute_reply.started":"2024-05-19T21:51:54.211384Z","shell.execute_reply":"2024-05-19T21:51:54.215538Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Method and Model Configuration\n# ---------------------------------------------------------\nentity = \"rstern\"\ndebugg = False\nmodel_name = 'filipealmeida/Mistral-7B-v0.1-sharded'\n# ---------------------------------------------------------\n# emb_org_a, emb_org_b, emb_aug_a, emb_aug_b\nexperiment_name = \"emb_org_a\" # model ## augmented_data ## author_label_only\naugmented_data = False # True: aug, False: org\nauthor_label_only = False # False: b and True: a\n# ---------------------------------------------------------\ndataset = 0 # 0, 1 or 2","metadata":{"execution":{"iopub.status.busy":"2024-05-19T21:48:25.201851Z","iopub.execute_input":"2024-05-19T21:48:25.203095Z","iopub.status.idle":"2024-05-19T21:48:25.208319Z","shell.execute_reply.started":"2024-05-19T21:48:25.203059Z","shell.execute_reply":"2024-05-19T21:48:25.207230Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"if dataset == 0:\n    test_df = pd.read_csv('/kaggle/input/lp-data/test_df_0.csv')\nelif dataset == 1:\n    test_df = pd.read_csv('/kaggle/input/lp-data/test_df_1.csv')\nelse:\n    test_df = pd.read_csv('/kaggle/input/lp-data/test_df_2.csv')\n\n# shuffle dataset\ntest_df = test_df.sample(frac=1, random_state=42)\nprint(test_df.sample(5))","metadata":{"execution":{"iopub.status.busy":"2024-05-19T21:48:27.705030Z","iopub.execute_input":"2024-05-19T21:48:27.705953Z","iopub.status.idle":"2024-05-19T21:48:27.745053Z","shell.execute_reply.started":"2024-05-19T21:48:27.705909Z","shell.execute_reply":"2024-05-19T21:48:27.744030Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"      Unnamed: 0                                         paragraph1  \\\n950         9215  >(c)In this section, the term “classified info...   \n933         8110  Its authoritarianism by definition. Control fr...   \n1357       10024  For anyone wondering why remnants of Nazi symb...   \n646         4898  Sure. you can call the cops. They'll take 15 m...   \n2127        2426  Putin doesn't care how many of his own country...   \n\n                                             paragraph2  label_author  \\\n950   It’s all just gross. I’m really sad about the ...             1   \n933   Outside of the major urban centers, functional...             1   \n1357  Which connections? Most of this stuff is publi...             1   \n646   r/politics is currently accepting new moderato...             1   \n2127  I don’t know how you know that. Yes, they are ...             1   \n\n      label_dataset  fileindex  \n950               0       3505  \n933               0       3095  \n1357              0       3806  \n646               0       1858  \n2127              0        923  \n","output_type":"stream"}]},{"cell_type":"code","source":"def create_sequences(row):\n  sequence = str(row[\"paragraph1\"]) + \"[LP2]\" + str(row[\"paragraph2\"])\n  return sequence\n\n# train_df[\"input\"] = train_df.apply(create_sequences, axis=1)\n# val_df[\"input\"] = val_df.apply(create_sequences, axis=1)\n\ndef create_multilabel(row):\n  label_dataset = max(min(1, row[\"label_dataset\"]), 0)\n  multilabel = np.array([int(row[\"label_author\"]), int(label_dataset)])\n  return multilabel\n\ndef create_singlelabel(row):\n  label = np.array([int(row[\"label_author\"])])\n  return label\n\nif author_label_only:\n  test_df[\"label\"] = test_df.apply(create_singlelabel, axis=1)\n  label_weights = [1]\nelse:\n  test_df[\"label\"] = test_df.apply(create_multilabel, axis=1)\n  # weight author label heavier than topic change label\n  label_weights = [2,1]\n\ny_test = test_df[\"label\"].values\ny_test = np.stack(y_test)\n\nx_test_par1 = test_df[\"paragraph1\"].values\nx_test_par2 = test_df[\"paragraph2\"].values\n","metadata":{"execution":{"iopub.status.busy":"2024-05-19T21:48:30.319691Z","iopub.execute_input":"2024-05-19T21:48:30.320269Z","iopub.status.idle":"2024-05-19T21:48:30.373035Z","shell.execute_reply.started":"2024-05-19T21:48:30.320236Z","shell.execute_reply":"2024-05-19T21:48:30.372080Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"ds = DatasetDict({\n    'test': Dataset.from_dict({'paragraph1': x_test_par1, 'paragraph2': x_test_par2, 'labels': y_test}),\n})","metadata":{"execution":{"iopub.status.busy":"2024-05-19T21:48:33.576927Z","iopub.execute_input":"2024-05-19T21:48:33.577313Z","iopub.status.idle":"2024-05-19T21:48:33.619976Z","shell.execute_reply.started":"2024-05-19T21:48:33.577284Z","shell.execute_reply":"2024-05-19T21:48:33.618908Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Initialize tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained('filipealmeida/Mistral-7B-v0.1-sharded')","metadata":{"execution":{"iopub.status.busy":"2024-05-19T21:48:36.799639Z","iopub.execute_input":"2024-05-19T21:48:36.800548Z","iopub.status.idle":"2024-05-19T21:48:37.854193Z","shell.execute_reply.started":"2024-05-19T21:48:36.800510Z","shell.execute_reply":"2024-05-19T21:48:37.853124Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/963 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b0660254d1243da9bb47530d91dea28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b10dc151e1e845d7b0709c1e2d014009"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47fe512b320c477ea944b19fa5f8ec77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6a127d2e19046339344e27863fb4f1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d25ccee7109849c395c03381e65056ac"}},"metadata":{}}]},{"cell_type":"code","source":"model = AutoModel.from_pretrained('filipealmeida/Mistral-7B-v0.1-sharded', load_in_4bit=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T21:48:40.061723Z","iopub.execute_input":"2024-05-19T21:48:40.062762Z","iopub.status.idle":"2024-05-19T21:50:45.689039Z","shell.execute_reply.started":"2024-05-19T21:48:40.062718Z","shell.execute_reply":"2024-05-19T21:50:45.688225Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/656 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e86e0548de242da982841dc4656a87a"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4020586a6c0748fb927c8938a2ae3588"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0d7480a32a14b4dbbf23611222c5593"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00008.bin:   0%|          | 0.00/1.89G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6e9a8bbfa1c4813aec8823effa04166"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00008.bin:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e7f2bdf64d5400a9159e55de12c3e11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00003-of-00008.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cc8280765ee4e4ba0d436a05696d73a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00004-of-00008.bin:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"376e4df5ecb848fba23df9d2c5038505"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00005-of-00008.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a4ff0f500a149ffb80596455408f4e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00006-of-00008.bin:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8babd29bb284142b64962ec9fd1a5a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00007-of-00008.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66b4b372e16a40f2b138a971994b3c4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00008-of-00008.bin:   0%|          | 0.00/816M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0c50143b8614800a42e4c9fa0490ff4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fad09d63b06c460ba21894049069d649"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"code","source":"def last_token_pool(last_hidden_states, attention_mask):\n  left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n  if left_padding:\n    return last_hidden_states[:, -1]\n  else:\n    sequence_lengths = attention_mask.sum(dim=1) - 1\n    batch_size = last_hidden_states.shape[0]\n    return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n\n\ndef collate_fn(batch):\n    max_length = 4096\n    embeddings1_list = []\n    embeddings2_list = []\n    labels_list = []\n\n    for sample in batch:\n        paragraph1 = sample['paragraph1']\n        paragraph2 = sample['paragraph2']\n        labels = sample['labels']\n        labels_list.append(labels)\n\n        inputs1 = tokenizer(paragraph1, max_length=max_length, truncation=True, return_tensors='pt')\n        inputs2 = tokenizer(paragraph2, max_length=max_length, truncation=True, return_tensors='pt')\n\n        with torch.no_grad():\n            outputs1 = model(**inputs1)\n            embeddings1 = last_token_pool(outputs1.last_hidden_state, inputs1['attention_mask'])\n\n            outputs2 = model(**inputs2)\n            embeddings2 = last_token_pool(outputs2.last_hidden_state, inputs2['attention_mask'])\n\n        embeddings1_list.append(embeddings1.float())\n        embeddings2_list.append(embeddings2.float())\n\n    embeddings1_batch = torch.stack(embeddings1_list, dim=0)\n    embeddings2_batch = torch.stack(embeddings2_list, dim=0)\n    labels_batch = torch.tensor(labels_list)\n\n    return embeddings1_batch, embeddings2_batch, labels_batch","metadata":{"execution":{"iopub.status.busy":"2024-05-19T21:50:49.227485Z","iopub.execute_input":"2024-05-19T21:50:49.227879Z","iopub.status.idle":"2024-05-19T21:50:49.240242Z","shell.execute_reply.started":"2024-05-19T21:50:49.227839Z","shell.execute_reply":"2024-05-19T21:50:49.238618Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Define the model architecture for the classifier\nclass Classifier(nn.Module):\n    def __init__(self, input_size, number_labels=1):\n        super(Classifier, self).__init__()\n        self.fc1 = nn.Linear(input_size * 2, 512)\n        self.fc2 = nn.Linear(512, 128)\n        self.fc3 = nn.Linear(128, number_labels)  # Assuming binary classification\n\n    def forward(self, emb1, emb2):\n        # Concatenate embeddings along the last dimension\n        x = torch.cat((emb1, emb2), dim=2)  # Shape: [batch_size, input_size * 2]\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n# define params\nif author_label_only:\n  number_labels = 1  # or 2 for binary classification with two labels\nelse:\n  number_labels = 2\nbatch_size = 16\ninput_size = 4096 # needs to be set as well above\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nloss_function = nn.BCEWithLogitsLoss()\n\nmodel_save_path = '/kaggle/input/lp-test-model/model.pth'","metadata":{"execution":{"iopub.status.busy":"2024-05-19T21:52:03.514815Z","iopub.execute_input":"2024-05-19T21:52:03.515734Z","iopub.status.idle":"2024-05-19T21:52:03.524587Z","shell.execute_reply.started":"2024-05-19T21:52:03.515682Z","shell.execute_reply":"2024-05-19T21:52:03.523533Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Instantiate the model\nclassifier = Classifier(input_size, number_labels)\n\n# Load the saved model weights\nclassifier.load_state_dict(torch.load(model_save_path))\n\n# Ensure the model is in evaluation mode\nclassifier.eval()\n\n# Move the model to the appropriate device\nclassifier.to(device)\n\n# get DataLoader\ntest_loader = DataLoader(ds['test'], batch_size=batch_size, collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T21:52:08.065342Z","iopub.execute_input":"2024-05-19T21:52:08.066055Z","iopub.status.idle":"2024-05-19T21:52:08.285730Z","shell.execute_reply.started":"2024-05-19T21:52:08.066020Z","shell.execute_reply":"2024-05-19T21:52:08.284900Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# List to store predictions and true labels\nall_predictions = []\nall_labels = []\nlosses = []\n\n# Disable gradient computation for evaluation\nwith torch.no_grad():\n    for batch_embeddings1, batch_embeddings2, target in test_loader:\n        batch_embeddings1 = batch_embeddings1.to(device).float()\n        batch_embeddings2 = batch_embeddings2.to(device).float()\n        target = target.to(device).float()\n\n        # Forward pass\n        output = classifier(batch_embeddings1, batch_embeddings2)\n\n        # Calculate the loss\n        if number_labels == 2:\n            loss = loss_function(output.squeeze(), target.float())\n        else:\n            loss = loss_function(output.squeeze(), target.squeeze().float())\n            losses.append(val_loss.item())\n        \n        # Apply sigmoid to get probabilities, then round to get binary predictions\n        # Calculate predictions\n        predictions = torch.sigmoid(output).round().squeeze()\n        all_predictions.extend(predictions.cpu().tolist())\n        all_labels.extend(target.cpu().tolist())\n        \n        # convert predictions and target to right format\n        predictions = torch.sigmoid(output).round().squeeze().cpu().tolist()\n        target = target.cpu().tolist()\n        \n        # calculate scores\n        train_f1_macro = f1_score(target, predictions, average=\"macro\")\n        \n        if number_labels == 2:\n            f1_macro_author = f1_score([[row[0]] for row in target], [[row[0]] for row in predictions])\n            print({\"loss\": loss, \"train_macro_f1\": train_f1_macro, \"train_macro_f1_author\": f1_macro_author})\n        else:\n            print({\"loss\": loss, \"train_macro_f1\": train_f1_macro})\n\nprint(\"Overall Results\")\n\n# Calculate validation metrics\nval_f1_macro = f1_score(val_targets, val_predictions, average=\"macro\")\nval_f1_micro = f1_score(val_targets, val_predictions, average=\"micro\")\nscores = {'epoch': epoch+1, 'loss': loss.item(), 'val_f1_macro': val_f1_macro, 'val_f1_micro': val_f1_micro}\nif number_labels == 2:\n    f1_macro_author = f1_score([[row[0]] for row in val_targets], [[row[0]] for row in val_predictions])\n    scores['val_f1_author'] = f1_macro_author\n    # Log validation metrics to wandb\nprint(scores)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T21:52:11.926622Z","iopub.execute_input":"2024-05-19T21:52:11.927020Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': tensor(1.0749, device='cuda:0'), 'train_macro_f1': 0.23809523809523808, 'train_macro_f1_author': 0.47619047619047616}\n{'loss': tensor(1.1980, device='cuda:0'), 'train_macro_f1': 0.15789473684210528, 'train_macro_f1_author': 0.31578947368421056}\n{'loss': tensor(0.8904, device='cuda:0'), 'train_macro_f1': 0.3181818181818182, 'train_macro_f1_author': 0.6363636363636364}\n{'loss': tensor(0.8606, device='cuda:0'), 'train_macro_f1': 0.41666666666666663, 'train_macro_f1_author': 0.8333333333333333}\n","output_type":"stream"}]}]}