{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8461562,"sourceType":"datasetVersion","datasetId":5034397}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Baseline using a simple CountVectorizer","metadata":{}},{"cell_type":"code","source":"import os, json\nimport pandas as pd\nimport numpy as np\n#!pip install --upgrade tensorflow\nimport tensorflow as tf\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import f1_score","metadata":{"execution":{"iopub.status.busy":"2024-05-23T15:49:17.770170Z","iopub.execute_input":"2024-05-23T15:49:17.770447Z","iopub.status.idle":"2024-05-23T15:49:23.881556Z","shell.execute_reply.started":"2024-05-23T15:49:17.770421Z","shell.execute_reply":"2024-05-23T15:49:23.880562Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-05-23 15:49:18.474967: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-23 15:49:18.475035: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-23 15:49:18.477107: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/lp-data/df_train.csv')\ndf_val = pd.read_csv('/kaggle/input/lp-data/df_validation.csv')\ndf_test = pd.read_csv('/kaggle/input/lp-data/df_test.csv')\n\n# shuffle dataset\ndf_train = df_train.sample(frac=1, random_state=42)\ndf_val = df_val.sample(frac=1, random_state=42)\ndf_test = df_test.sample(frac=1, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T15:49:23.883497Z","iopub.execute_input":"2024-05-23T15:49:23.884570Z","iopub.status.idle":"2024-05-23T15:49:24.440075Z","shell.execute_reply.started":"2024-05-23T15:49:23.884531Z","shell.execute_reply":"2024-05-23T15:49:24.439063Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df_train_bal = pd.read_csv('/kaggle/input/lp-data/balanced_train.csv')\ndf_val_bal = pd.read_csv('/kaggle/input/lp-data/balanced_val.csv')\ndf_test_bal = pd.read_csv('/kaggle/input/lp-data/balanced_test.csv')\n\n# shuffle dataset\ndf_train_bal = df_train_bal.sample(frac=1, random_state=42)\ndf_val_bal = df_val_bal.sample(frac=1, random_state=42)\ndf_test_bal = df_test_bal.sample(frac=1, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(df_train))\nprint(len(df_train_bal))\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### Configurations\n\n- emb_org_a: original data, author_label only\n- emb_org_b: orignal data, author_label and dataset_label\n- emb_aug_a: augmented_data, author_label only\n- emb_aug_b: augmented_data, author_label and dataset_label \n\n## 1) Training and Saving models for emb_org_a and emb_aug_a (only author_label)","metadata":{}},{"cell_type":"code","source":"# 1) labels for emb_org_a\n\n# convert labels to numerical values\nlabel_encoder = LabelEncoder()\ny_train = label_encoder.fit_transform(df_train['label_author'])\ny_val = label_encoder.transform(df_val['label_author'])\ny_test = label_encoder.transform(df_test['label_author'])\n\n# get bow representation of paragraph1\nvectorizer_par1 = CountVectorizer(max_features=5000)\nX_train_par1 = vectorizer_par1.fit_transform(df_train['paragraph1']).toarray()\nX_val_par1 = vectorizer_par1.transform(df_val['paragraph1']).toarray()\nX_test_par1 = vectorizer_par1.transform(df_test['paragraph1']).toarray()\n\n# get bow representation of paragraph2\nvectorizer_par2 = CountVectorizer(max_features=5000)\nX_train_par2 = vectorizer_par2.fit_transform(df_train['paragraph2']).toarray()\nX_val_par2 = vectorizer_par2.transform(df_val['paragraph2']).toarray()\nX_test_par2 = vectorizer_par2.transform(df_test['paragraph2']).toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2) labels for emb_aug_a\n\n# convert labels to numerical values\nlabel_encoder_bal = LabelEncoder()\ny_train_bal = label_encoder_bal.fit_transform(df_train_bal['label_author'])\ny_val_bal = label_encoder_bal.transform(df_val_bal['label_author'])\ny_test_bal = label_encoder_bal.transform(df_test_bal['label_author'])\n\n# get bow representation of paragraph1\nvectorizer_par1_bal = CountVectorizer(max_features=5000)\nX_train_par1_bal = vectorizer_par1_bal.fit_transform(df_train_bal['paragraph1']).toarray()\nX_val_par1_bal = vectorizer_par1_bal.transform(df_val_bal['paragraph1']).toarray()\nX_test_par1_bal = vectorizer_par1_bal.transform(df_test_bal['paragraph1']).toarray()\n\n# get bow representation of paragraph2\nvectorizer_par2_bal = CountVectorizer(max_features=5000)\nX_train_par2_bal = vectorizer_par2_bal.fit_transform(df_train_bal['paragraph2']).toarray()\nX_val_par2_bal = vectorizer_par2_bal.transform(df_val_bal['paragraph2']).toarray()\nX_test_par2_bal = vectorizer_par2_bal.transform(df_test_bal['paragraph2']).toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the features for training, validation, and test  sets\n# 1) emb_org_a\nX_train = np.concatenate((X_train_par1, X_train_par2), axis=1)\nX_val = np.concatenate((X_val_par1, X_val_par2), axis=1)\nX_test = np.concatenate((X_test_par1, X_test_par2), axis=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#2) emb_aug_a\n#X_train_bal = torch.from_numpy(np.concatenate((X_train_par1_bal, X_train_par2_bal), axis=1)).to('cuda')\n#X_val_bal = torch.from_numpy(np.concatenate((X_val_par1_bal, X_val_par2_bal), axis=1)).to('cuda')\n#X_test_bal = torch.from_numpy(np.concatenate((X_test_par1_bal, X_test_par2_bal), axis=1)).to('cuda')\nX_train_bal = np.concatenate((X_train_par1_bal, X_train_par2_bal), axis=1)\nX_val_bal = np.concatenate((X_val_par1_bal, X_val_par2_bal), axis=1)\nX_test_bal = np.concatenate((X_test_par1_bal, X_test_par2_bal), axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# emb_org_a\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_val, y_val))\n\n# evaluate on validation set\nval_loss, val_accuracy = model.evaluate(X_val, y_val)\nprint(f'Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}')\n\n# evaluate on test set \ntest_loss, test_accuracy = model.evaluate(X_test, y_test)\nprint(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n\n# --------- F1 Score ---------------- \n\n# make predictions needed for F1 score\ny_val_pred = (model.predict(X_val) > 0.5).astype(\"int32\")\ny_test_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n\n# calculate F1 macro\nval_f1_macro = f1_score(y_val, y_val_pred, average='macro')\ntest_f1_macro = f1_score(y_test, y_test_pred, average='macro')\n\nprint(f'Validation F1-Macro: {val_f1_macro}')\nprint(f'Test F1-Macro: {test_f1_macro}')\n\n# -------------------------\n\n#test_predictions = model.predict(X_test) # should be list of style changes\nmodel.save(\"baseline_model_emb_org_a.h5\") # author label only ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# emb_aug_a, data set is so big that it allocates too much memory. TO DO: train with smaller data set of augmented data or push to cuda\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train_bal.shape[1],)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nhistory = model.fit(X_train_bal, y_train_bal, epochs=10, batch_size=16, validation_data=(X_val_bal, y_val_bal))\n\n# evaluate on validation set\nval_loss, val_accuracy = model.evaluate(X_val_bal, y_val_bal)\nprint(f'Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}')\n\n# evaluate on test set \ntest_loss, test_accuracy = model.evaluate(X_test_bal, y_test_bal)\nprint(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n\n# --------- F1 Score ---------------- actually not needed because dataset is balanced\n\n# make predictions needed for F1 score\ny_val_pred = (model.predict(X_val_bal) > 0.5).astype(\"int32\")\ny_test_pred = (model.predict(X_test_bal) > 0.5).astype(\"int32\")\n\n# calculate F1 macro\nval_f1_macro = f1_score(y_val_bal, y_val_pred_bal, average='macro')\ntest_f1_macro = f1_score(y_test_bal, y_test_pred_bal, average='macro')\n\nprint(f'Validation F1-Macro: {val_f1_macro}')\nprint(f'Test F1-Macro: {test_f1_macro}')\n\n# -------------------------\n\n\n#test_predictions = model.predict(X_test) # should be list of style changes\nmodel.save(\"baseline_model_emb_aug_a.h5\") # author label only","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2) Training and Saving models for emb_org_b and emb_aug_b (Multi-Label Classification for label_author and label_dataset)","metadata":{}},{"cell_type":"code","source":"# 1) embed_org_b\n\n# Encode the labels\nlabel_encoder_author = LabelEncoder()\nlabel_encoder_dataset = LabelEncoder()\n\ny_train_author = label_encoder_author.fit_transform(df_train['label_author'])\ny_val_author = label_encoder_author.transform(df_val['label_author'])\ny_test_author = label_encoder_author.transform(df_test['label_author'])\n\ny_train_dataset = label_encoder_dataset.fit_transform(df_train['label_dataset'])\ny_val_dataset = label_encoder_dataset.transform(df_val['label_dataset'])\ny_test_dataset = label_encoder_dataset.transform(df_test['label_dataset'])\n\n\n# stack labels to create multilabel\ny_train_multilabel = np.column_stack((y_train_author, y_train_dataset))\ny_val_multilabel = np.column_stack((y_val_author, y_val_dataset))\ny_test_multilabel = np.column_stack((y_test_author, y_test_dataset))\n\n\n# Get BOW representation of paragraph1\nvectorizer_par1 = CountVectorizer(max_features=5000)\nX_train_par1 = vectorizer_par1.fit_transform(df_train['paragraph1']).toarray()\nX_val_par1 = vectorizer_par1.transform(df_val['paragraph1']).toarray()\nX_test_par1 = vectorizer_par1.transform(df_test['paragraph1']).toarray()\n\n# Get BOW representation of paragraph2\nvectorizer_par2 = CountVectorizer(max_features=5000)\nX_train_par2 = vectorizer_par2.fit_transform(df_train['paragraph2']).toarray()\nX_val_par2 = vectorizer_par2.transform(df_val['paragraph2']).toarray()\nX_test_par2 = vectorizer_par2.transform(df_test['paragraph2']).toarray()\n\n# Concatenate paragraph1 and paragraph2 BOW representations\nX_train = np.concatenate((X_train_par1, X_train_par2), axis=1)\nX_val = np.concatenate((X_val_par1, X_val_par2), axis=1)\nX_test = np.concatenate((X_test_par1, X_test_par2), axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T15:49:33.734220Z","iopub.execute_input":"2024-05-23T15:49:33.735083Z","iopub.status.idle":"2024-05-23T15:49:44.892486Z","shell.execute_reply.started":"2024-05-23T15:49:33.735046Z","shell.execute_reply":"2024-05-23T15:49:44.891360Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(y_train_multilabel.shape)\n\nprint(X_val.shape)\nprint(y_val_multilabel.shape)\nprint(y_train_multilabel[0:5])","metadata":{"execution":{"iopub.status.busy":"2024-05-23T15:49:44.894776Z","iopub.execute_input":"2024-05-23T15:49:44.895175Z","iopub.status.idle":"2024-05-23T15:49:44.901115Z","shell.execute_reply.started":"2024-05-23T15:49:44.895139Z","shell.execute_reply":"2024-05-23T15:49:44.900067Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"(41594, 10000)\n(41594, 2)\n(11198, 10000)\n(11198, 2)\n[[0 1]\n [1 1]\n [0 2]\n [0 2]\n [1 0]]\n","output_type":"stream"}]},{"cell_type":"code","source":"input_shape = X_train.shape[1]\n\ninput_layer = tf.keras.layers.Input(shape=(input_shape,))\ndense1 = tf.keras.layers.Dense(128, activation='relu')(input_layer)\ndense2 = tf.keras.layers.Dense(64, activation='relu')(dense1)\n\n# Binary label output layer\nauthor_output = tf.keras.layers.Dense(1, activation='sigmoid', name='author_output')(dense2)\n\n# Multiclass label output layer\ndataset_output = tf.keras.layers.Dense(3, activation='softmax', name='dataset_output')(dense2)\n\n\n# output = tf.keras.layers.Dense(2, activation='softmax', name='output')(dense2) # for 1 stacked multilabel\n#model = tf.keras.models.Model(inputs=input_layer, outputs=output) # for 1 stacked multilabel\n\nmodel = tf.keras.models.Model(inputs=input_layer, outputs=[author_output, dataset_output])\n\nmodel.compile(optimizer='adam',\n              loss={'author_output': 'binary_crossentropy', 'dataset_output': 'categorical_crossentropy'},\n              metrics=['accuracy', 'accuracy'])\n\n'''#Compile the model\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])''' # for 1 stacked multilabel\n\n# train model\nhistory = model.fit(X_train, {'author_output': y_train_author, 'dataset_output': tf.keras.utils.to_categorical(y_train_dataset)},\n                    epochs=10, batch_size=16,\n                    validation_data=(X_val, {'author_output': y_val_author, 'dataset_output': tf.keras.utils.to_categorical(y_val_dataset)}))\n\n#history = model.fit(X_train, y_train_multilabel, epochs=10, batch_size=16,\n                  #  validation_data=(X_val, y_val_multilabel)) # for 1 stacked multilabel\n\n\n# Save the model\nmodel.save(\"baseline_model_emb_org_b.h5\")","metadata":{"execution":{"iopub.status.busy":"2024-05-23T15:49:54.075685Z","iopub.execute_input":"2024-05-23T15:49:54.076611Z","iopub.status.idle":"2024-05-23T15:51:23.701778Z","shell.execute_reply.started":"2024-05-23T15:49:54.076534Z","shell.execute_reply":"2024-05-23T15:51:23.699920Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m  68/2600\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - author_output_accuracy: 0.6586 - dataset_output_accuracy: 0.4884 - loss: 1.6529","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1716479405.195668    2075 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nW0000 00:00:1716479405.210981    2075 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2600/2600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - author_output_accuracy: 0.6549 - dataset_output_accuracy: 0.5683 - loss: 1.4333","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1716479414.726413    2075 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2600/2600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 4ms/step - author_output_accuracy: 0.6549 - dataset_output_accuracy: 0.5683 - loss: 1.4333 - val_author_output_accuracy: 0.6667 - val_dataset_output_accuracy: 0.6021 - val_loss: 1.3336\nEpoch 2/10\n\u001b[1m2600/2600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - author_output_accuracy: 0.7458 - dataset_output_accuracy: 0.7233 - loss: 1.0842 - val_author_output_accuracy: 0.6638 - val_dataset_output_accuracy: 0.5923 - val_loss: 1.4709\nEpoch 3/10\n\u001b[1m2600/2600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - author_output_accuracy: 0.8776 - dataset_output_accuracy: 0.8720 - loss: 0.6058 - val_author_output_accuracy: 0.6450 - val_dataset_output_accuracy: 0.5733 - val_loss: 1.9760\nEpoch 4/10\n\u001b[1m2600/2600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - author_output_accuracy: 0.9642 - dataset_output_accuracy: 0.9549 - loss: 0.2417 - val_author_output_accuracy: 0.6453 - val_dataset_output_accuracy: 0.5644 - val_loss: 2.7981\nEpoch 5/10\n\u001b[1m2600/2600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - author_output_accuracy: 0.9875 - dataset_output_accuracy: 0.9807 - loss: 0.1128 - val_author_output_accuracy: 0.6474 - val_dataset_output_accuracy: 0.5547 - val_loss: 3.6616\nEpoch 6/10\n\u001b[1m2600/2600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - author_output_accuracy: 0.9940 - dataset_output_accuracy: 0.9875 - loss: 0.0690 - val_author_output_accuracy: 0.6557 - val_dataset_output_accuracy: 0.5647 - val_loss: 4.2099\nEpoch 7/10\n\u001b[1m2600/2600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - author_output_accuracy: 0.9948 - dataset_output_accuracy: 0.9893 - loss: 0.0575 - val_author_output_accuracy: 0.6493 - val_dataset_output_accuracy: 0.5644 - val_loss: 4.6818\nEpoch 8/10\n\u001b[1m2600/2600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - author_output_accuracy: 0.9955 - dataset_output_accuracy: 0.9918 - loss: 0.0432 - val_author_output_accuracy: 0.6497 - val_dataset_output_accuracy: 0.5632 - val_loss: 4.9928\nEpoch 9/10\n\u001b[1m2600/2600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - author_output_accuracy: 0.9973 - dataset_output_accuracy: 0.9920 - loss: 0.0356 - val_author_output_accuracy: 0.6477 - val_dataset_output_accuracy: 0.5743 - val_loss: 5.6465\nEpoch 10/10\n\u001b[1m2600/2600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - author_output_accuracy: 0.9977 - dataset_output_accuracy: 0.9912 - loss: 0.0341 - val_author_output_accuracy: 0.6574 - val_dataset_output_accuracy: 0.5541 - val_loss: 5.7335\n","output_type":"stream"}]},{"cell_type":"code","source":"# TO DO: Evaluate on validation set\n\n''''val_loss, val_author_loss, val_dataset_loss, val_author_accuracy, val_dataset_accuracy = model.evaluate(X_val, [y_val_author, y_val_dataset], verbose=2)\nprint(f'Validation Loss: {val_loss}, Validation Author Loss: {val_author_loss}, Validation Dataset Loss: {val_dataset_loss}')\nprint(f'Validation Author Accuracy: {val_author_accuracy}, Validation Dataset Accuracy: {val_dataset_accuracy}')\n\n# Evaluate on test set\ntest_loss, test_author_loss, test_dataset_loss, test_author_accuracy, test_dataset_accuracy = model.evaluate(X_test, [y_test_author, y_test_dataset], verbose=2)\nprint(f'Test Loss: {test_loss}, Test Author Loss: {test_author_loss}, Test Dataset Loss: {test_dataset_loss}')\nprint(f'Test Author Accuracy: {test_author_accuracy}, Test Dataset Accuracy: {test_dataset_accuracy}')\n\n# Make predictions needed for F1 score\ny_val_pred_author = (model.predict(X_val)[0] > 0.5).astype(\"int32\")\ny_val_pred_dataset = (model.predict(X_val)[1] > 0.5).astype(\"int32\")\n\ny_test_pred_author = (model.predict(X_test)[0] > 0.5).astype(\"int32\")\ny_test_pred_dataset = (model.predict(X_test)[1] > 0.5).astype(\"int32\")\n\n# Calculate F1 macro\nval_f1_macro_author = f1_score(y_val_author, y_val_pred_author, average='macro')\nval_f1_macro_dataset = f1_score(y_val_dataset, y_val_pred_dataset, average='macro')\n\ntest_f1_macro_author = f1_score(y_test_author, y_test_pred_author, average='macro')\ntest_f1_macro_dataset = f1_score(y_test_dataset, y_test_pred_dataset, average='macro')\n\nprint(f'Validation Author F1-Macro: {val_f1_macro_author}')\nprint(f'Validation Dataset F1-Macro: {val_f1_macro_dataset}')\nprint(f'Test Author F1-Macro: {test_f1_macro_author}')\nprint(f'Test Dataset F1-Macro: {test_f1_macro_dataset}')'''","metadata":{"execution":{"iopub.status.busy":"2024-05-23T15:58:48.959531Z","iopub.execute_input":"2024-05-23T15:58:48.960653Z","iopub.status.idle":"2024-05-23T15:58:48.968818Z","shell.execute_reply.started":"2024-05-23T15:58:48.960613Z","shell.execute_reply":"2024-05-23T15:58:48.967863Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'\\'val_loss, val_author_loss, val_dataset_loss, val_author_accuracy, val_dataset_accuracy = model.evaluate(X_val, [y_val_author, y_val_dataset], verbose=2)\\nprint(f\\'Validation Loss: {val_loss}, Validation Author Loss: {val_author_loss}, Validation Dataset Loss: {val_dataset_loss}\\')\\nprint(f\\'Validation Author Accuracy: {val_author_accuracy}, Validation Dataset Accuracy: {val_dataset_accuracy}\\')\\n\\n# Evaluate on test set\\ntest_loss, test_author_loss, test_dataset_loss, test_author_accuracy, test_dataset_accuracy = model.evaluate(X_test, [y_test_author, y_test_dataset], verbose=2)\\nprint(f\\'Test Loss: {test_loss}, Test Author Loss: {test_author_loss}, Test Dataset Loss: {test_dataset_loss}\\')\\nprint(f\\'Test Author Accuracy: {test_author_accuracy}, Test Dataset Accuracy: {test_dataset_accuracy}\\')\\n\\n# Make predictions needed for F1 score\\ny_val_pred_author = (model.predict(X_val)[0] > 0.5).astype(\"int32\")\\ny_val_pred_dataset = (model.predict(X_val)[1] > 0.5).astype(\"int32\")\\n\\ny_test_pred_author = (model.predict(X_test)[0] > 0.5).astype(\"int32\")\\ny_test_pred_dataset = (model.predict(X_test)[1] > 0.5).astype(\"int32\")\\n\\n# Calculate F1 macro\\nval_f1_macro_author = f1_score(y_val_author, y_val_pred_author, average=\\'macro\\')\\nval_f1_macro_dataset = f1_score(y_val_dataset, y_val_pred_dataset, average=\\'macro\\')\\n\\ntest_f1_macro_author = f1_score(y_test_author, y_test_pred_author, average=\\'macro\\')\\ntest_f1_macro_dataset = f1_score(y_test_dataset, y_test_pred_dataset, average=\\'macro\\')\\n\\nprint(f\\'Validation Author F1-Macro: {val_f1_macro_author}\\')\\nprint(f\\'Validation Dataset F1-Macro: {val_f1_macro_dataset}\\')\\nprint(f\\'Test Author F1-Macro: {test_f1_macro_author}\\')\\nprint(f\\'Test Dataset F1-Macro: {test_f1_macro_dataset}\\')'"},"metadata":{}}]}]}