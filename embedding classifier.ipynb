{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2e8bb5-b55f-4017-8b18-1eb519f97933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import functools\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import PeftModel, PeftConfig\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "# let's log every trained\n",
    "# %env WANDB_LOG_MODEL=true\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81c1195-04b3-4f8d-8646-2b51f34718ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method and Model Configuration\n",
    "# ---------------------------------------------------------\n",
    "entity = \"rstern\"\n",
    "debugg = False\n",
    "model_name = 'mistralai/Mistral-7B-v0.1'\n",
    "# ---------------------------------------------------------\n",
    "model_config = \"embeddings\" # possible values: org_model, frozen\n",
    "augmented_data = True # True: aug, False: org\n",
    "author_label_only = True # False: b and True: a\n",
    "experiment_name = \"emb_aug_a\" # model_config augmented_data author_label_only\n",
    "# model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d37725-4740-4401-a1f3-7e52536ce23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "  from google.colab import drive\n",
    "  import sys\n",
    "  drive.mount('/content/drive')\n",
    "  # sys.path.append('/content/drive/MyDrive/ucph/LP Project') # If working in collab change this path\n",
    "  path = '/content/drive/MyDrive/ucph/LP Project/'\n",
    "  if augmented_data:\n",
    "    train_df = pd.read_csv(f'{path}balanced_train.csv')\n",
    "    val_df = pd.read_csv(f'{path}balanced_val.csv')\n",
    "  else:\n",
    "    train_df = pd.read_csv(f'{path}df_train.csv')\n",
    "    val_df = pd.read_csv(f'{path}df_validation.csv')\n",
    "else:\n",
    "    if augmented_data:\n",
    "        train_df = pd.read_csv(f'balanced_train.csv')\n",
    "        val_df = pd.read_csv(f'balanced_val.csv')\n",
    "    else:\n",
    "        train_df = pd.read_csv(f'df_train.csv')\n",
    "        val_df = pd.read_csv(f'df_validation.csv')\n",
    "\n",
    "# shuffle dataset\n",
    "train_df = train_df.sample(frac=1, random_state=42)\n",
    "val_df = val_df.sample(frac=1, random_state=42)\n",
    "print(train_df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70528073-f3c1-4a8f-9a20-1245dd612da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multilabel(row):\n",
    "  label_dataset = max(min(1, row[\"label_dataset\"]), 0)\n",
    "  multilabel = np.array([int(row[\"label_author\"]), int(label_dataset)])\n",
    "  return multilabel\n",
    "\n",
    "def create_singlelabel(row):\n",
    "  label = np.array([int(row[\"label_author\"])])\n",
    "  return label\n",
    "\n",
    "if author_label_only:\n",
    "  train_df[\"label\"] = train_df.apply(create_singlelabel, axis=1)\n",
    "  val_df[\"label\"] = val_df.apply(create_singlelabel, axis=1)\n",
    "  label_weights = [1]\n",
    "else:\n",
    "  train_df[\"label\"] = train_df.apply(create_multilabel, axis=1)\n",
    "  val_df[\"label\"] = val_df.apply(create_multilabel, axis=1)\n",
    "  # weight author label heavier than topic change label\n",
    "  label_weights = [2,1]\n",
    "\n",
    "y_train = train_df[\"label\"].values\n",
    "y_train = np.stack(y_train)\n",
    "y_val = val_df[\"label\"].values\n",
    "y_val = np.stack(y_val)\n",
    "\n",
    "x_train_par1 = train_df[\"paragraph1\"].values\n",
    "x_train_par2 = train_df[\"paragraph2\"].values\n",
    "\n",
    "x_val_par1 = val_df[\"paragraph1\"].values\n",
    "x_val_par2 = val_df[\"paragraph2\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d919a4-dc9b-4c36-a00e-91480928852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds = DatasetDict({\n",
    "    'train': Dataset.from_dict({'paragraph1': x_train_par1, 'paragraph2': x_train_par2, 'labels': y_train}),\n",
    "    'val': Dataset.from_dict({'paragraph1': x_val_par1, 'paragraph2': x_val_par2, 'labels': y_train})\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaaaa80-e883-4e3b-a5c6-157754620578",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-v0.1')\n",
    "model = AutoModel.from_pretrained('mistralai/Mistral-7B-v0.1')\n",
    "# model.to('cuda')\n",
    "\n",
    "max_length = 4096\n",
    "\n",
    "batch_dict = tokenizer(documents, max_length=max_length - 1, return_attention_mask=False, padding=False, truncation=True)\n",
    "\n",
    "batch_dict['input_ids'] = [input_ids + [tokenizer.eos_token_id] for input_ids in batch_dict['input_ids']]\n",
    "batch_dict = tokenizer.pad(batch_dict, padding=True, return_attention_mask=True, return_tensors='pt')\n",
    "# batch_dict.to('cuda')\n",
    "\n",
    "outputs = model(**batch_dict)\n",
    "embeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "\n",
    "embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "embeddings = embeddings.tolist()\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "# Define the model architecture for the classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size * 2, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)  # Assuming binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def last_token_pool(last_hidden_states, attention_mask):\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-v0.1')\n",
    "model = AutoModel.from_pretrained('mistralai/Mistral-7B-v0.1')\n",
    "\n",
    "# Define batch size and other parameters\n",
    "batch_size = 16\n",
    "max_length = 4096\n",
    "\n",
    "# Prepare DataLoader for training and validation\n",
    "def collate_fn(batch):\n",
    "    inputs1 = tokenizer(batch['paragraph1'], padding='max_length', max_length=max_length, truncation=True, return_tensors='pt')\n",
    "    inputs2 = tokenizer(batch['paragraph2'], padding='max_length', max_length=max_length, truncation=True, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs1 = model(**inputs1)\n",
    "        embeddings1 = last_token_pool(outputs1.last_hidden_state, inputs1['attention_mask'])\n",
    "        \n",
    "        outputs2 = model(**inputs2)\n",
    "        embeddings2 = last_token_pool(outputs2.last_hidden_state, inputs2['attention_mask'])\n",
    "    \n",
    "    return embeddings1, embeddings2\n",
    "\n",
    "train_loader = DataLoader(ds['train'], batch_size=batch_size, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(ds['val'], batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "# Instantiate the classifier model\n",
    "classifier = Classifier(model.config.hidden_size)\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project='lp2-embeddings', entity='entity')\n",
    "\n",
    "# Log hyperparameters\n",
    "config = wandb.config\n",
    "config.batch_size = batch_size\n",
    "config.max_length = max_length\n",
    "\n",
    "# Training loop\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "classifier.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    classifier.train()\n",
    "    for batch_embeddings1, batch_embeddings2 in train_loader:\n",
    "        batch_embeddings1 = batch_embeddings1.to(device)\n",
    "        batch_embeddings2 = batch_embeddings2.to(device)\n",
    "        \n",
    "        output = classifier(torch.cat((batch_embeddings1, batch_embeddings2), dim=1))\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, labels)  # You need to define labels\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation loop\n",
    "    classifier.eval()\n",
    "    val_labels = []\n",
    "    val_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch_embeddings1, batch_embeddings2 in val_loader:\n",
    "            batch_embeddings1 = batch_embeddings1.to(device)\n",
    "            batch_embeddings2 = batch_embeddings2.to(device)\n",
    "            \n",
    "            output = classifier(torch.cat((batch_embeddings1, batch_embeddings2), dim=1))\n",
    "            val_labels.extend(batch['labels'].cpu().numpy())\n",
    "            val_predictions.extend(torch.argmax(output, dim=1).cpu().numpy())\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    val_f1 = f1_score(val_labels, val_predictions, average='macro')\n",
    "    \n",
    "    # Log metrics to wandb\n",
    "    wandb.log({'epoch': epoch+1, 'loss': loss.item(), 'val_f1': val_f1})\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Validation F1: {val_f1}')\n",
    "\n",
    "# Finish wandb run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
