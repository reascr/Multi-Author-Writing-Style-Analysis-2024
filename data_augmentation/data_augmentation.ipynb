{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb4e23d3",
   "metadata": {},
   "source": [
    "### Data Augmentation and Oversampling\n",
    "\n",
    "The idea is to leverage the metadata and to follow the algorithm proposed by Hashemi et al. (2023) to get pairs of non-consecutive paragraphs (with and\n",
    "without style changes). Then, classes will be oversampled to obtain a balanced data set.\n",
    "\n",
    "Description of the algorithm: \" incorporate additional non-consecutive pairs of paragraphs\n",
    "into our sample set and assign them labels based on the inferred relationships. For example, if\n",
    "there are three consecutive paragraphs without a style change, we can infer that the first and\n",
    "third paragraphs are written by the same author. Similarly, if there are style changes between\n",
    "the first and second paragraphs and between the second and third paragraphs, we can deduce\n",
    "that the authors of the first and third paragraphs are different, given that the number of authors\n",
    "in the document exceeds the number of style changes by one.\" (Hashemi et al. 2023: 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61d49e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e43bf86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Val split is: 0.82-0.18\n",
      "Number of rows where label_author == 0 (training data): 20485\n",
      "Number of rows where label_author == 1 (training data): 31508\n",
      "Number of rows where label_author == 0 (validation data): 4489\n",
      "Number of rows where label_author == 1 (validation data): 6709\n",
      "Number of duplicate rows in balanced training dataset: 7957\n",
      "Number of duplicate rows in balanced validation dataset: 1526\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = '../data_pipeline/'\n",
    "\n",
    "# get data sets \n",
    "df_train = pd.read_csv(os.path.join(BASE_DIR, \"df_train.csv\"), index_col=0)\n",
    "df_val = pd.read_csv(os.path.join(BASE_DIR, \"df_validation.csv\"), index_col=0)\n",
    "\n",
    "# check distribution of labels\n",
    "changes_train = len(df_train[df_train['label_author'] == 1])\n",
    "no_changes_train = len(df_train[df_train['label_author'] == 0])\n",
    "\n",
    "changes_val = len(df_val[df_val['label_author'] == 1])\n",
    "no_changes_val = len(df_val[df_val['label_author'] == 0])\n",
    "\n",
    "total_data = len(df_train) + len(df_val)\n",
    "train_ratio = len(df_train) / total_data\n",
    "val_ratio = len(df_val) / total_data\n",
    "\n",
    "print(f\"Train-Val split is: {train_ratio:.2f}-{val_ratio:.2f}\")\n",
    "\n",
    "\n",
    "print(f\"Number of rows where label_author == 0 (training data): {no_changes_train}\")\n",
    "print(f\"Number of rows where label_author == 1 (training data): {changes_train}\")\n",
    "\n",
    "print(f\"Number of rows where label_author == 0 (validation data): {no_changes_val}\")\n",
    "print(f\"Number of rows where label_author == 1 (validation data): {changes_val}\")\n",
    "\n",
    "# check for duplicates\n",
    "\n",
    "duplicate_train = df_train.duplicated().sum()\n",
    "\n",
    "# Check for duplicates in the balanced validation dataset\n",
    "duplicate_val = df_val.duplicated().sum()\n",
    "\n",
    "print(f\"Number of duplicate rows in balanced training dataset: {duplicate_rows_train}\")\n",
    "print(f\"Number of duplicate rows in balanced validation dataset: {duplicate_rows_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50f1013e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n data_train: 51993\n",
      "created 47598 additional training data.\n",
      "n data_val: 11198\n",
      "created 11532 additional validation data.\n",
      "New Train-Val split is: 0.81-0.19\n"
     ]
    }
   ],
   "source": [
    "def data_augmentation(data):\n",
    "    augmented_rows = []\n",
    "    # problem is that fileindexes start from 1 for every label_dataset --> iterate over label_dataset first\n",
    "    unique_datasets = data['label_dataset'].unique()\n",
    "    for dataset in unique_datasets:\n",
    "        dataset_data = data[data['label_dataset'] == dataset] # get subset of data for easy, medium, and hard\n",
    "        unique_fileindexes = data['fileindex'].unique() # get unique fileindexes\n",
    "        for file_index in unique_fileindexes:\n",
    "            file_data = dataset_data[dataset_data['fileindex'] == file_index] # get DataFrame for file\n",
    "            \n",
    "            if (file_data['label_author'] == 1).sum() == (file_data[\"n_authors\"].iloc[0] - 1):\n",
    "                for i in range(len(file_data)-1):\n",
    "                    row = file_data.iloc[i]\n",
    "                    j = i + 1 # set next paragraph index\n",
    "\n",
    "                    while (j < len(file_data)) and (file_data[\"label_author\"].iloc[j-1] == 0):# while same author\n",
    "                        if j > i:\n",
    "                            augmented_rows.append({\n",
    "                        'paragraph1': row['paragraph1'],\n",
    "                        'paragraph2': file_data['paragraph2'].iloc[j],\n",
    "                        'label_author': 0, # same author\n",
    "                        'label_dataset': row['label_dataset'],\n",
    "                        'n_authors': row['n_authors'],\n",
    "                        'fileindex': row['fileindex']\n",
    "                    })   \n",
    "                        j +=1 # move to next paragraph\n",
    "                    while j < len(file_data):\n",
    "                        if j > i:\n",
    "                            augmented_rows.append({\n",
    "                        'paragraph1': row['paragraph1'],\n",
    "                        'paragraph2': file_data['paragraph2'].iloc[j],\n",
    "                        'label_author': 1, # style change\n",
    "                        'label_dataset': row['label_dataset'],\n",
    "                        'n_authors': row['n_authors'],\n",
    "                        'fileindex': row['fileindex']\n",
    "                    })\n",
    "                        j += 1 # move to next paragraph\n",
    "            \n",
    "    # Create a new DataFrame with augmented rows\n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    return augmented_df\n",
    "\n",
    "augmented_df_train = data_augmentation(df_train)\n",
    "print(f\"n data_train: {len(df_train)}\")\n",
    "print(f\"created {len(augmented_df_train)} additional training data.\")\n",
    "augmented_df_train[:10]\n",
    "\n",
    "augmented_df_val = data_augmentation(df_val)\n",
    "print(f\"n data_val: {len(df_val)}\")\n",
    "print(f\"created {len(augmented_df_val)} additional validation data.\")\n",
    "augmented_df_val[:10]\n",
    "\n",
    "add_training_df = pd.concat([df_train, augmented_df_train], ignore_index=True)\n",
    "add_val_df = pd.concat([df_val, augmented_df_val], ignore_index=True)\n",
    "\n",
    "\n",
    "# write into new files and drop column for n_authors\n",
    "add_training_df.drop(columns=['n_authors']).to_csv('training_data_augmented.csv', index=True)\n",
    "\n",
    "add_val_df.drop(columns=['n_authors']).to_csv('validation_data_augmented.csv', index=True)\n",
    "\n",
    "# new train-val split\n",
    "\n",
    "total_data_new = len(add_training_df) + len(add_val_df)\n",
    "train_ratio_new = len(add_training_df) / total_data_new\n",
    "val_ratio_new = len(add_val_df) / total_data_new\n",
    "\n",
    "print(f\"New Train-Val split is: {train_ratio_new:.2f}-{val_ratio_new:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25335e0",
   "metadata": {},
   "source": [
    "### Balancing data set (create two new files for balanced data sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2516ee95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where label_author == 0 (training data): 36791\n",
      "Number of rows where label_author == 1 (training data): 62800\n",
      "Number of rows where label_author == 0 (validation data): 8599\n",
      "Number of rows where label_author == 1 (validation data): 14131\n",
      "\n",
      "Created 141.57% more training and 152.38% more validation data through creating new paragraph pairs and swapping paragraphs to oversample minority classes.\n",
      "\n",
      "Train-Val split is: 0.82-0.18\n"
     ]
    }
   ],
   "source": [
    "augmented_train = pd.read_csv(\"training_data_augmented.csv\", index_col=0)\n",
    "augmented_val = pd.read_csv(\"validation_data_augmented.csv\", index_col=0)\n",
    "\n",
    "\n",
    "# check distribution of labels\n",
    "changes_train_aug = len(add_training_df[add_training_df['label_author'] == 1])\n",
    "no_changes_train_aug = len(add_training_df[add_training_df['label_author'] == 0])\n",
    "\n",
    "changes_val_aug = len(add_val_df[add_val_df['label_author'] == 1])\n",
    "no_changes_val_aug = len(add_val_df[add_val_df['label_author'] == 0])\n",
    "\n",
    "print(f\"Number of rows where label_author == 0 (training data): {no_changes_train_aug}\")\n",
    "print(f\"Number of rows where label_author == 1 (training data): {changes_train_aug}\")\n",
    "\n",
    "print(f\"Number of rows where label_author == 0 (validation data): {no_changes_val_aug}\")\n",
    "print(f\"Number of rows where label_author == 1 (validation data): {changes_val_aug}\")\n",
    "\n",
    "\n",
    "# original train-validation split 0.82-0.12\n",
    "\n",
    "def balance_data(train,val):\n",
    "    '''balance data by swapping paragraph1 and paragraph2 and oversample the minority class'''\n",
    "    \n",
    "    total_data = len(train) + len(val)\n",
    "    ratio_train = len(train) / total_data\n",
    "    ratio_val = len(val) / total_data\n",
    "    \n",
    "    change_train = len(train[train['label_author'] == 1])\n",
    "    no_change_train = len(train[train['label_author'] == 0]) # minority class\n",
    "    change_val = len(val[val['label_author'] == 1])\n",
    "    no_change_val = len(val[val['label_author'] == 0]) # minority class\n",
    "    \n",
    "    balanced_rows_train = []\n",
    "    balanced_rows_val = []\n",
    "    \n",
    "    for i in range(change_train-no_change_train):\n",
    "        row = train[train['label_author'] == 0].sample(n=1, replace=False).iloc[0] # take random sample without replacement from rows with no changes\n",
    "        balanced_rows_train.append({\n",
    "                        'paragraph1': row['paragraph2'],\n",
    "                        'paragraph2': row['paragraph1'],\n",
    "                        'label_author': 0, # no style change (minority class) \n",
    "                        'label_dataset': row['label_dataset'],\n",
    "                        'fileindex': row['fileindex']\n",
    "                    })\n",
    "    \n",
    "    for i in range(change_val-no_change_val):\n",
    "        row = val[val['label_author'] == 0].sample(n=1, replace=False).iloc[0]\n",
    "        balanced_rows_val.append({\n",
    "                        'paragraph1': row['paragraph2'],\n",
    "                        'paragraph2': row['paragraph1'],\n",
    "                        'label_author': 0, # no style change (minority class) \n",
    "                        'label_dataset': row['label_dataset'],\n",
    "                        'fileindex': row['fileindex']\n",
    "                    })\n",
    "    \n",
    "    balanced_train = pd.DataFrame(balanced_rows_train)\n",
    "    balanced_val = pd.DataFrame(balanced_rows_val)\n",
    "    return balanced_train, balanced_val\n",
    "\n",
    "balanced_train, balanced_val = balance_data(augmented_train, augmented_val)\n",
    "\n",
    "balanced_train = pd.concat([augmented_train, balanced_train], ignore_index=True)\n",
    "balanced_val = pd.concat([augmented_val, balanced_val], ignore_index=True)\n",
    "\n",
    "# write into new files and drop column for n_authors\n",
    "balanced_train.to_csv('balanced_train.csv', index=True)\n",
    "balanced_val.to_csv('balanced_val.csv', index=True)\n",
    "\n",
    "increase_train = (len(balanced_train) - len(df_train)) / len(df_train) * 100\n",
    "increase_val = (len(balanced_val) - len(df_val)) / len(df_val) * 100\n",
    "\n",
    "print()\n",
    "print(f\"Created {increase_train:.2f}% more training and {increase_val:.2f}% more validation data through creating new paragraph pairs and swapping paragraphs to oversample minority classes.\")\n",
    "\n",
    "total_data = len(balanced_train) + len(balanced_val)\n",
    "train_ratio = len(balanced_train) / total_data\n",
    "val_ratio = len(balanced_val) / total_data\n",
    "\n",
    "print()\n",
    "print(f\"Train-Val split is: {train_ratio:.2f}-{val_ratio:.2f}\") # ensure split stays the same (actually unnecessary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66b29838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51993\n",
      "99591\n",
      "125600\n",
      "Number of duplicate rows in balanced training dataset: 7957\n",
      "Number of duplicate rows in balanced validation dataset: 1526\n"
     ]
    }
   ],
   "source": [
    "# DELETE THIS\n",
    "print(len(df_train)) # original training data\n",
    "print(len(add_training_df)) # after adding new paragraphs \n",
    "print(len(balanced_train)) # after balancing classes by oversampling minority class (no change)\n",
    "\n",
    "# Check for duplicates in the balanced training dataset\n",
    "duplicate_rows_train = balanced_train.duplicated().sum()\n",
    "\n",
    "# Check for duplicates in the balanced validation dataset\n",
    "duplicate_rows_val = balanced_val.duplicated().sum()\n",
    "\n",
    "print(f\"Number of duplicate rows in balanced training dataset: {duplicate_rows_train}\")\n",
    "print(f\"Number of duplicate rows in balanced validation dataset: {duplicate_rows_val}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
