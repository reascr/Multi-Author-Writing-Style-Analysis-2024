{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb4e23d3",
   "metadata": {},
   "source": [
    "### Data Augmentation and Oversampling\n",
    "\n",
    "The idea is to leverage the metadata and following the alogrithm proposed by Hashemi et al. (2023) to get pairs of non-consecutive paragraphs (with and\n",
    "without style changes). Then, classes will be oversampled to obtain a balanced data set.\n",
    "\n",
    "Description of algorithm: \" incorporate additional non-consecutive pairs of paragraphs\n",
    "into our sample set and assign them labels based on the inferred relationships. For example, if\n",
    "there are three consecutive paragraphs without a style change, we can infer that the first and\n",
    "third paragraphs are written by the same author. Similarly, if there are style changes between\n",
    "the first and second paragraphs and between the second and third paragraphs, we can deduce\n",
    "that the authors of the first and third paragraphs are different, given that the number of authors\n",
    "in the document exceeds the number of style changes by one.\" (Hashemi et al. 2023: 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61d49e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e43bf86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where label_author == 0 (training data): 20485\n",
      "Number of rows where label_author == 1 (training data): 31508\n",
      "Number of rows where label_author == 0 (validation data): 4489\n",
      "Number of rows where label_author == 1 (validation data): 6709\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = '../data_pipeline/'\n",
    "\n",
    "# get data sets \n",
    "df_train = pd.read_csv(os.path.join(BASE_DIR, \"df_train.csv\"))\n",
    "df_val = pd.read_csv(os.path.join(BASE_DIR, \"df_validation.csv\"))\n",
    "\n",
    "# check distribution of labels\n",
    "changes_train = len(df_train[df_train['label_author'] == 1])\n",
    "no_changes_train = len(df_train[df_train['label_author'] == 0])\n",
    "\n",
    "changes_val = len(df_val[df_val['label_author'] == 1])\n",
    "no_changes_val = len(df_val[df_val['label_author'] == 0])\n",
    "\n",
    "print(f\"Number of rows where label_author == 0 (training data): {no_changes_train}\")\n",
    "print(f\"Number of rows where label_author == 1 (training data): {changes_train}\")\n",
    "\n",
    "print(f\"Number of rows where label_author == 0 (validation data): {no_changes_val}\")\n",
    "print(f\"Number of rows where label_author == 1 (validation data): {changes_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50f1013e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         True\n",
      "1         True\n",
      "2         True\n",
      "3         True\n",
      "4         True\n",
      "11065     True\n",
      "32978     True\n",
      "32979    False\n",
      "32980     True\n",
      "Name: label_author, dtype: bool\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m augmented_df\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m augmented_df \u001b[38;5;241m=\u001b[39m \u001b[43mdata_augmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m augmented_df\n",
      "Cell \u001b[0;32mIn[26], line 9\u001b[0m, in \u001b[0;36mdata_augmentation\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_index \u001b[38;5;129;01min\u001b[39;00m unique_fileindexes:\n\u001b[1;32m      7\u001b[0m     file_data \u001b[38;5;241m=\u001b[39m data[data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfileindex\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m file_index] \u001b[38;5;66;03m# get DataFrame for file\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel_author\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m() \u001b[38;5;66;03m# problem here! Because some dont have 1s\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (file_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_author\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m==\u001b[39m (file_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_authors\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m file_data\u001b[38;5;241m.\u001b[39miterrows():\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sum'"
     ]
    }
   ],
   "source": [
    "#for i in range(data[fileindex].loc[-1]): # iterate over fileindexes \n",
    "def data_augmentation(data):\n",
    "    augmented_rows = []\n",
    "    unique_fileindexes = data['fileindex'].unique()\n",
    "    \n",
    "    for file_index in unique_fileindexes:\n",
    "        file_data = data[data['fileindex'] == file_index] # get DataFrame for file\n",
    "\n",
    "        print(file_data['label_author'] == 1).sum() # problem here! Because some dont have 1s\n",
    "        if (file_data['label_author'] == 1).sum() == (file_data[\"n_authors\"].iloc[0] - 1):\n",
    "            for index, row in file_data.iterrows():\n",
    "                j = index + 1 # set next paragraph index\n",
    "                while j < len(file_data) and file_data[\"label_author\"].iloc[j-1] == 0:\n",
    "                    if j > i + 1:\n",
    "                        augmented_rows.append({\n",
    "                    'paragraph1': row['paragraph1'],\n",
    "                    'paragraph2': file_data.loc[index + 1, 'paragraph1'],\n",
    "                    'label_author': 0, # same author\n",
    "                    'label_dataset': row['label_dataset'],\n",
    "                    'n_authors': row['n_authors'],\n",
    "                    'fileindex': row['fileindex']\n",
    "                })\n",
    "                    j +=1 # move to next paragraph\n",
    "                while j < len(file_data):\n",
    "                    if j > i + 1:\n",
    "                        augmented_rows.append({\n",
    "                    'paragraph1': row['paragraph1'],\n",
    "                    'paragraph2': file_data.loc[index + 1, 'paragraph1'],\n",
    "                    'label_author': 1, # style change\n",
    "                    'label_dataset': row['label_dataset'],\n",
    "                    'n_authors': row['n_authors'],\n",
    "                    'fileindex': row['fileindex']\n",
    "                })\n",
    "                    j += 1 # move to next paragraph\n",
    "            \n",
    "    # Create a new DataFrame with augmented rows\n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    #augmented_df = pd.concat([data, augmented_df], ignore_index=True)\n",
    "\n",
    "    return augmented_df\n",
    "\n",
    "# Example usage:\n",
    "augmented_df = data_augmentation(df_train)\n",
    "augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d150558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_order(data):\n",
    "    '''double the training data by swapping paragraph1 and paragraph2'''\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
