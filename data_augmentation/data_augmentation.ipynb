{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb4e23d3",
   "metadata": {},
   "source": [
    "### Data Augmentation and Oversampling\n",
    "\n",
    "The idea is to leverage the metadata and to follow the algorithm proposed by Hashemi et al. (2023) to get pairs of non-consecutive paragraphs (with and\n",
    "without style changes). Then, classes will be oversampled to obtain a balanced data set.\n",
    "\n",
    "Description of the algorithm: \" incorporate additional non-consecutive pairs of paragraphs\n",
    "into our sample set and assign them labels based on the inferred relationships. For example, if\n",
    "there are three consecutive paragraphs without a style change, we can infer that the first and\n",
    "third paragraphs are written by the same author. Similarly, if there are style changes between\n",
    "the first and second paragraphs and between the second and third paragraphs, we can deduce\n",
    "that the authors of the first and third paragraphs are different, given that the number of authors\n",
    "in the document exceeds the number of style changes by one.\" (Hashemi et al. 2023: 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61d49e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e43bf86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where label_author == 0 (training data): 20485\n",
      "Number of rows where label_author == 1 (training data): 31508\n",
      "Number of rows where label_author == 0 (validation data): 4489\n",
      "Number of rows where label_author == 1 (validation data): 6709\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = '../data_pipeline/'\n",
    "\n",
    "# get data sets \n",
    "df_train = pd.read_csv(os.path.join(BASE_DIR, \"df_train.csv\"))\n",
    "df_val = pd.read_csv(os.path.join(BASE_DIR, \"df_validation.csv\"))\n",
    "\n",
    "# check distribution of labels\n",
    "changes_train = len(df_train[df_train['label_author'] == 1])\n",
    "no_changes_train = len(df_train[df_train['label_author'] == 0])\n",
    "\n",
    "changes_val = len(df_val[df_val['label_author'] == 1])\n",
    "no_changes_val = len(df_val[df_val['label_author'] == 0])\n",
    "\n",
    "print(f\"Number of rows where label_author == 0 (training data): {no_changes_train}\")\n",
    "print(f\"Number of rows where label_author == 1 (training data): {changes_train}\")\n",
    "\n",
    "print(f\"Number of rows where label_author == 0 (validation data): {no_changes_val}\")\n",
    "print(f\"Number of rows where label_author == 1 (validation data): {changes_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f1013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(data):\n",
    "    augmented_rows = []\n",
    "    unique_fileindexes = data['fileindex'].unique()\n",
    "    \n",
    "    for file_index in unique_fileindexes:\n",
    "        file_data = data[data['fileindex'] == file_index] # get DataFrame for file\n",
    "\n",
    "        print(file_data['label_author'] == 1).sum() # problem here! Because some rows dont have 1s\n",
    "        if (file_data['label_author'] == 1).sum() == (file_data[\"n_authors\"].iloc[0] - 1):\n",
    "            for index, row in file_data.iterrows():\n",
    "                j = index + 1 # set next paragraph index\n",
    "                while j < len(file_data) and file_data[\"label_author\"].iloc[j-1] == 0:\n",
    "                    if j > i + 1:\n",
    "                        augmented_rows.append({\n",
    "                    'paragraph1': row['paragraph1'],\n",
    "                    'paragraph2': file_data.loc[index + 1, 'paragraph1'],\n",
    "                    'label_author': 0, # same author\n",
    "                    'label_dataset': row['label_dataset'],\n",
    "                    'n_authors': row['n_authors'],\n",
    "                    'fileindex': row['fileindex']\n",
    "                })\n",
    "                    j +=1 # move to next paragraph\n",
    "                while j < len(file_data):\n",
    "                    if j > i + 1:\n",
    "                        augmented_rows.append({\n",
    "                    'paragraph1': row['paragraph1'],\n",
    "                    'paragraph2': file_data.loc[index + 1, 'paragraph1'],\n",
    "                    'label_author': 1, # style change\n",
    "                    'label_dataset': row['label_dataset'],\n",
    "                    'n_authors': row['n_authors'],\n",
    "                    'fileindex': row['fileindex']\n",
    "                })\n",
    "                    j += 1 # move to next paragraph\n",
    "            \n",
    "    # Create a new DataFrame with augmented rows\n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    #augmented_df = pd.concat([data, augmented_df], ignore_index=True)\n",
    "\n",
    "    return augmented_df\n",
    "\n",
    "#augmented_df = data_augmentation(df_train)\n",
    "#augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d150558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_order(data):\n",
    "    '''double the training data by swapping paragraph1 and paragraph2'''\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
