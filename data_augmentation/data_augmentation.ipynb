{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb4e23d3",
   "metadata": {},
   "source": [
    "### Data Augmentation and Oversampling\n",
    "\n",
    "The idea is to leverage the metadata and to follow the algorithm proposed by Hashemi et al. (2023) to get pairs of non-consecutive paragraphs (with and\n",
    "without style changes). Then, classes will be oversampled to obtain a balanced data set.\n",
    "\n",
    "Description of the algorithm: \" incorporate additional non-consecutive pairs of paragraphs\n",
    "into our sample set and assign them labels based on the inferred relationships. For example, if\n",
    "there are three consecutive paragraphs without a style change, we can infer that the first and\n",
    "third paragraphs are written by the same author. Similarly, if there are style changes between\n",
    "the first and second paragraphs and between the second and third paragraphs, we can deduce\n",
    "that the authors of the first and third paragraphs are different, given that the number of authors\n",
    "in the document exceeds the number of style changes by one.\" (Hashemi et al. 2023: 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61d49e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e43bf86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where label_author == 0 (training data): 20485\n",
      "Number of rows where label_author == 1 (training data): 31508\n",
      "Number of rows where label_author == 0 (validation data): 4489\n",
      "Number of rows where label_author == 1 (validation data): 6709\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = '../data_pipeline/'\n",
    "\n",
    "# get data sets \n",
    "df_train = pd.read_csv(os.path.join(BASE_DIR, \"df_train.csv\"))\n",
    "df_val = pd.read_csv(os.path.join(BASE_DIR, \"df_validation.csv\"))\n",
    "\n",
    "# check distribution of labels\n",
    "changes_train = len(df_train[df_train['label_author'] == 1])\n",
    "no_changes_train = len(df_train[df_train['label_author'] == 0])\n",
    "\n",
    "changes_val = len(df_val[df_val['label_author'] == 1])\n",
    "no_changes_val = len(df_val[df_val['label_author'] == 0])\n",
    "\n",
    "print(f\"Number of rows where label_author == 0 (training data): {no_changes_train}\")\n",
    "print(f\"Number of rows where label_author == 1 (training data): {changes_train}\")\n",
    "\n",
    "print(f\"Number of rows where label_author == 0 (validation data): {no_changes_val}\")\n",
    "print(f\"Number of rows where label_author == 1 (validation data): {changes_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50f1013e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n data_train: 51993\n",
      "created 47598 additional training data.\n",
      "n data_train: 11198\n",
      "created 11532 additional training data.\n"
     ]
    }
   ],
   "source": [
    "def data_augmentation(data):\n",
    "    augmented_rows = []\n",
    "    # problem is that fileindexes start from 1 for every label_dataset --> iterate over label_dataset first\n",
    "    unique_datasets = data['label_dataset'].unique()\n",
    "    for dataset in unique_datasets:\n",
    "        dataset_data = data[data['label_dataset'] == dataset] # get subset of data for easy, medium, and hard\n",
    "        unique_fileindexes = data['fileindex'].unique() # get unique fileindexes\n",
    "        for file_index in unique_fileindexes:\n",
    "            file_data = dataset_data[dataset_data['fileindex'] == file_index] # get DataFrame for file\n",
    "            \n",
    "            if (file_data['label_author'] == 1).sum() == (file_data[\"n_authors\"].iloc[0] - 1):\n",
    "                for i in range(len(file_data)-1):\n",
    "                    row = file_data.iloc[i]\n",
    "                    j = i + 1 # set next paragraph index\n",
    "\n",
    "                    while (j < len(file_data)) and (file_data[\"label_author\"].iloc[j-1] == 0):# while same author\n",
    "                        # Hashemi: if j > i + 1 \n",
    "                        if j > i:\n",
    "                            augmented_rows.append({\n",
    "                        'paragraph1': row['paragraph1'],\n",
    "                        'paragraph2': file_data['paragraph2'].iloc[j],\n",
    "                        'label_author': 0, # same author\n",
    "                        'label_dataset': row['label_dataset'],\n",
    "                        'n_authors': row['n_authors'],\n",
    "                        'fileindex': row['fileindex']\n",
    "                    })   \n",
    "                        j +=1 # move to next paragraph\n",
    "                    while j < len(file_data):\n",
    "                        #if j > i + 1:\n",
    "                        if j > i:\n",
    "                        # Hashemi: if j > i + 1\n",
    "                            augmented_rows.append({\n",
    "                        'paragraph1': row['paragraph1'],\n",
    "                        'paragraph2': file_data['paragraph2'].iloc[j],\n",
    "                        'label_author': 1, # style change\n",
    "                        'label_dataset': row['label_dataset'],\n",
    "                        'n_authors': row['n_authors'],\n",
    "                        'fileindex': row['fileindex']\n",
    "                    })\n",
    "                        j += 1 # move to next paragraph\n",
    "            \n",
    "    # Create a new DataFrame with augmented rows\n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    return augmented_df\n",
    "\n",
    "augmented_df_train = data_augmentation(df_train)\n",
    "print(f\"n data_train: {len(df_train)}\")\n",
    "print(f\"created {len(augmented_df_train)} additional training data.\")\n",
    "augmented_df_train[:10]\n",
    "\n",
    "augmented_df_val = data_augmentation(df_val)\n",
    "print(f\"n data_train: {len(df_val)}\")\n",
    "print(f\"created {len(augmented_df_val)} additional training data.\")\n",
    "augmented_df_val[:10]\n",
    "\n",
    "add_training_df = pd.concat([df_train, augmented_df_train], ignore_index=True)\n",
    "add_val_df = pd.concat([df_val, augmented_df_val], ignore_index=True)\n",
    "\n",
    "\n",
    "# write into new files and drop column for n_author\n",
    "add_training_df.drop(columns=['n_authors']).to_csv('training_data_augmented.csv', index=False)\n",
    "\n",
    "add_val_df.drop(columns=['n_authors']).to_csv('validation_data_augmented.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4468cc",
   "metadata": {},
   "source": [
    "#### I changed the algorithm of Hashemi et al. (2023) in the two if statements in the while loops to obtain even more training data.\n",
    "We could think of implementing the following to double the number of training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d150558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_order(data):\n",
    "    '''double the training data by swapping paragraph1 and paragraph2'''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dba849d",
   "metadata": {},
   "source": [
    "### Balancing data set (create two new files for balanced data sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6d72d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
