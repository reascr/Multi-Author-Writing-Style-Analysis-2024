Originally left and right was a question of Democracy vs Monarchy in France. It had nothing to do with capitalism. Throughout history the left has been used to describe liberalism and since the Democratic party is a liberal party, the term is appropriate. After the growth of movements such as Marxism and anarchism, the term "left" became more commonly used to describe people of those philosophies with the original "left" liberals and other left-leaning capitalists being rebranded as center-left (which I am perfectly fine with.) But to be perfectly clear, center-left is still left and no mainstream definition of the term puts social liberals or the Democratic party on the right.
They might be center righteous, but the Democrats as a whole are globally on the left side of the political spectrum. They believe in an expanded government that regulates corporations and serves the people, they support social welfare like healthcare and food stamps, fund education, protect the environment, and put a great deal of focus on making a more free and equal country.
Biden is the man. He'll hand you a butterscotch with one hand and give you the finger with the other. He has not one fuck to give and I'm here for it.
r/politics is currently accepting new moderator applications. If you want to help make this community a better place, consider !