Physicists have since the 1950s sought to harness the fusion reaction that powers the sun, but no group had been able to produce more energy from the reaction than it consumes â€“ a milestone known as net energy gain or target gain.
Converting heat from fusion to electricity (which is not done at the moment) would have an efficiency of maybe 30% or so.
Assuming ITER reaches its goal it won't be that far away: 300 MW electricity used to produce 50 MW of plasma heating which leads to 500 MW of fusion, converting that to electricity would give something like 150-200 MW. There is also some power for cooling and other stuff, but we are talking about a factor 2-3 to break-even instead of the 300 for inertial confinement. In addition it's already in a relevant power range - going from 500 MW to 3 GW is a smaller step than going from a shot per day to multiple shots per second.
First they compared the output to the energy absorbed by the fuel for a first round of news when they exceeded that.
isn't it net energy gain compared to the energy beams that hit the fuel pellet? DOE labs measure it that way and apparently the rest of the world doesn't? Which would be hugely misleading regarding break even point unless we are to just ignore all the missed energy inputs.
is it though? At this institution, don't they measure the energy input by the amount absorbed by the fuel pellet? Which means all the missed energy beams just straight up don't count, according to this lab. apparently this is not the standard way to do this.