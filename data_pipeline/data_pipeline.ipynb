{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for creating input data for the model, TO DO: apply to the cells below\n",
    "\n",
    "def create_input_data(input_dir):\n",
    "    datasets = {\n",
    "    'easy': 0,\n",
    "    'medium': 1,\n",
    "    'hard': 2,\n",
    "}\n",
    "    BASE_DIR = input_dir \n",
    "\n",
    "    data = []\n",
    "    #columns = ['paragraph1', 'paragraph2', 'label_author', 'label_dataset', 'fileindex']\n",
    "    columns = ['paragraph1', 'paragraph2', 'label_author', 'label_dataset', 'n_authors', 'fileindex']\n",
    "\n",
    "    # iterate over all the files in the directory\n",
    "    for difficulty, difficulty_label in datasets.items():\n",
    "        directory = os.path.join(BASE_DIR, difficulty, 'train')\n",
    "        nfiles = len(os.listdir(directory))/2\n",
    "        print(f'dataset: {difficulty}, #files: {nfiles}')\n",
    "        i = 1\n",
    "        while i <= nfiles:\n",
    "            problem = os.path.join(directory, f'problem-{i}.txt')\n",
    "            solution = os.path.join(directory, f'truth-problem-{i}.json')\n",
    "\n",
    "            with open(problem) as f_problem, open(solution) as f_solution:\n",
    "                # read all paragraphs\n",
    "                paragraphs = [str.strip(line) for line in f_problem.readlines()]\n",
    "\n",
    "                # read solution\n",
    "                sol_data = json.load(f_solution)\n",
    "                sol = sol_data['changes']\n",
    "                #sol = json.load(f_solution)['changes'] # changed this to read both changes and n_authors\n",
    "                n_authors = sol_data['authors']\n",
    "                #print(f\"file {i}, ds {difficulty}, sol {sol}, number of authors {n_authors}\")\n",
    "\n",
    "                try:\n",
    "                    # assign consecutive paragraphs to label\n",
    "                    for j in range(len(paragraphs)-1):\n",
    "                        para1 = paragraphs[j]\n",
    "                        para2 = paragraphs[j+1]\n",
    "                        solution_label = sol[j]\n",
    "\n",
    "                        # dump into dataset\n",
    "                        data.append([\n",
    "                            para1, para2, solution_label, difficulty_label, n_authors, i\n",
    "                        ])\n",
    "                except:\n",
    "                    print(f'file {i} has some problems')\n",
    "            i += 1\n",
    "        print(f'last file elaborated for {difficulty} was number {i}')\n",
    "\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    df.to_csv('df_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: easy, #files: 4200.0\n",
      "file 1082 has some problems\n",
      "file 2525 has some problems\n",
      "last file elaborated for easy was number 4201\n",
      "dataset: medium, #files: 4200.0\n",
      "file 1033 has some problems\n",
      "last file elaborated for medium was number 4201\n",
      "dataset: hard, #files: 4200.0\n",
      "file 244 has some problems\n",
      "file 639 has some problems\n",
      "last file elaborated for hard was number 4201\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "\n",
    "datasets = {\n",
    "    'easy': 0,\n",
    "    'medium': 1,\n",
    "    'hard': 2,\n",
    "}\n",
    "# BASE_DIR = './pan24-multi-author-analysis' \n",
    "BASE_DIR = '../pan24-multi-author-analysis' \n",
    "\n",
    "data = []\n",
    "#columns = ['paragraph1', 'paragraph2', 'label_author', 'label_dataset', 'fileindex']\n",
    "columns = ['paragraph1', 'paragraph2', 'label_author', 'label_dataset', 'n_authors', 'fileindex']\n",
    "\n",
    "# iterate over all the files in the directory\n",
    "for difficulty, difficulty_label in datasets.items():\n",
    "    directory = os.path.join(BASE_DIR, difficulty, 'train')\n",
    "    nfiles = len(os.listdir(directory))/2\n",
    "    print(f'dataset: {difficulty}, #files: {nfiles}')\n",
    "    i = 1\n",
    "    while i <= nfiles:\n",
    "        problem = os.path.join(directory, f'problem-{i}.txt')\n",
    "        solution = os.path.join(directory, f'truth-problem-{i}.json')\n",
    "\n",
    "        with open(problem) as f_problem, open(solution) as f_solution:\n",
    "            # read all paragraphs\n",
    "            paragraphs = [str.strip(line) for line in f_problem.readlines()]\n",
    "\n",
    "            # read solution\n",
    "            sol_data = json.load(f_solution)\n",
    "            sol = sol_data['changes']\n",
    "            #sol = json.load(f_solution)['changes'] # changed this to read both changes and n_authors\n",
    "            n_authors = sol_data['authors']\n",
    "            #print(f\"file {i}, ds {difficulty}, sol {sol}, number of authors {n_authors}\")\n",
    "\n",
    "            try:\n",
    "                # assign consecutive paragraphs to label\n",
    "                for j in range(len(paragraphs)-1):\n",
    "                    para1 = paragraphs[j]\n",
    "                    para2 = paragraphs[j+1]\n",
    "                    solution_label = sol[j]\n",
    "                    \n",
    "                    # dump into dataset\n",
    "                    data.append([\n",
    "                        para1, para2, solution_label, difficulty_label, n_authors, i\n",
    "                    ])\n",
    "            except:\n",
    "                print(f'file {i} has some problems')\n",
    "        i += 1\n",
    "    print(f'last file elaborated for {difficulty} was number {i}')\n",
    "\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df.to_csv('df_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: easy, #files: 900.0\n",
      "last file elaborated for easy was number 901\n",
      "dataset: medium, #files: 900.0\n",
      "last file elaborated for medium was number 901\n",
      "dataset: hard, #files: 900.0\n",
      "file 475 has some problems\n",
      "last file elaborated for hard was number 901\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "\n",
    "datasets = {\n",
    "    'easy': 0,\n",
    "    'medium': 1,\n",
    "    'hard': 2,\n",
    "}\n",
    "BASE_DIR = './pan24-multi-author-analysis'\n",
    "BASE_DIR = '../pan24-multi-author-analysis'\n",
    "\n",
    "\n",
    "data = []\n",
    "columns = ['paragraph1', 'paragraph2', 'label_author', 'label_dataset', 'fileindex']\n",
    "\n",
    "# iterate over all the files in the directory\n",
    "for difficulty, difficulty_label in datasets.items():\n",
    "    directory = os.path.join(BASE_DIR, difficulty, 'validation')\n",
    "    nfiles = len(os.listdir(directory))/2\n",
    "    print(f'dataset: {difficulty}, #files: {nfiles}')\n",
    "    i = 1\n",
    "    while i <= nfiles:\n",
    "        problem = os.path.join(directory, f'problem-{i}.txt')\n",
    "        solution = os.path.join(directory, f'truth-problem-{i}.json')\n",
    "\n",
    "        with open(problem) as f_problem, open(solution) as f_solution:\n",
    "            # read all paragraphs\n",
    "            paragraphs = [str.strip(line) for line in f_problem.readlines()]\n",
    "\n",
    "            # read solution\n",
    "            sol = json.load(f_solution)['changes']\n",
    "            # print(f\"file {i}, ds {difficulty}, sol {sol}\")\n",
    "\n",
    "            try:\n",
    "                # assign consecutive paragraphs to label\n",
    "                for j in range(len(paragraphs)-1):\n",
    "                    para1 = paragraphs[j]\n",
    "                    para2 = paragraphs[j+1]\n",
    "                    solution_label = sol[j]\n",
    "                    \n",
    "                    # dump into dataset\n",
    "                    data.append([\n",
    "                        para1, para2, solution_label, difficulty_label, i\n",
    "                    ])\n",
    "            except:\n",
    "                print(f'file {i} has some problems')\n",
    "        i += 1\n",
    "    print(f'last file elaborated for {difficulty} was number {i}')\n",
    "\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df.to_csv('df_validation.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
